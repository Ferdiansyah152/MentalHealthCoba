{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962af853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import skops.io as sio\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_suite import DataDriftSuite\n",
    "from evidently.metrics import DatasetDriftMetric, DatasetMissingValuesMetric, DatasetCorrelationsMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Setup direktori yang diperlukan\"\"\"\n",
    "    directories = [\"models\", \"results\", \"monitoring\", \"monitoring/evidently_reports\"]\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    print(\"✅ Directories setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load dan prepare data dengan Evidently monitoring integration\"\"\"\n",
    "    print(\"🔍 Loading and preparing data...\")\n",
    "    \n",
    "    # Path data files yang konsisten dengan evidently_monitoring.py\n",
    "    data_files = [\n",
    "        \"data/mental_health_lite.csv\", \n",
    "        \"data/mental_health_life_cut.csv\"\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    data_source = None\n",
    "    \n",
    "    # Cari file data yang tersedia\n",
    "    for file_path in data_files:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"✅ Found data file: {file_path}\")\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                data_source = file_path\n",
    "                print(f\"✅ Data loaded successfully from {file_path}\")\n",
    "                print(f\"📊 Data shape: {df.shape}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error loading {file_path}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if df is None:\n",
    "        raise FileNotFoundError(\"❌ No valid dataset found in data/ folder\")\n",
    "    \n",
    "    # Create data loading summary untuk monitoring\n",
    "    data_summary = {\n",
    "        \"data_loaded\": True,\n",
    "        \"source_file\": data_source,\n",
    "        \"shape\": df.shape,\n",
    "        \"columns\": list(df.columns),\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"missing_values\": df.isnull().sum().to_dict(),\n",
    "        \"data_types\": df.dtypes.astype(str).to_dict()\n",
    "    }\n",
    "    \n",
    "    # Save data summary\n",
    "    with open(\"monitoring/data_loading_summary.json\", \"w\") as f:\n",
    "        json.dump(data_summary, f, indent=2, default=str)\n",
    "    \n",
    "    return df, data_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe74a4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evidently_data_report(df, data_source):\n",
    "    \"\"\"Create Evidently data quality report\"\"\"\n",
    "    print(\"📊 Creating Evidently data quality report...\")\n",
    "    \n",
    "    try:\n",
    "        # Create basic data quality report\n",
    "        data_report = Report(metrics=[\n",
    "            DatasetMissingValuesMetric(),\n",
    "            DatasetCorrelationsMetric(),\n",
    "        ])\n",
    "        \n",
    "        # Run report\n",
    "        data_report.run(reference_data=None, current_data=df)\n",
    "        \n",
    "        # Save HTML report\n",
    "        report_path = \"monitoring/evidently_reports/data_quality_report.html\"\n",
    "        data_report.save_html(report_path)\n",
    "        \n",
    "        # Extract key metrics\n",
    "        report_dict = data_report.as_dict()\n",
    "        \n",
    "        data_quality_summary = {\n",
    "            \"report_generated\": True,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"missing_values_count\": df.isnull().sum().sum(),\n",
    "            \"report_path\": report_path\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(data_quality_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"✅ Evidently data quality report saved to {report_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Evidently data quality report failed: {e}\")\n",
    "        # Create fallback summary\n",
    "        fallback_summary = {\n",
    "            \"report_generated\": False,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"data_source\": data_source,\n",
    "            \"total_rows\": len(df),\n",
    "            \"total_columns\": len(df.columns),\n",
    "            \"missing_values_count\": df.isnull().sum().sum()\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/evidently_data_quality.json\", \"w\") as f:\n",
    "            json.dump(fallback_summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(df):\n",
    "    \"\"\"Encode categorical features dengan proper handling\"\"\"\n",
    "    print(\"🔧 Encoding categorical features...\")\n",
    "    \n",
    "    encoders = {}\n",
    "    \n",
    "    # Identifikasi kolom kategorik\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Exclude target column jika ada\n",
    "    target_candidates = ['mental_health_condition', 'target', 'label', 'class']\n",
    "    target_column = None\n",
    "    \n",
    "    for col in target_candidates:\n",
    "        if col in df.columns:\n",
    "            target_column = col\n",
    "            if col in categorical_columns:\n",
    "                categorical_columns.remove(col)\n",
    "            break\n",
    "    \n",
    "    print(f\"🎯 Target column identified: {target_column}\")\n",
    "    print(f\"📝 Categorical columns to encode: {categorical_columns}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            encoder = LabelEncoder()\n",
    "            # Handle missing values\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "            df[f\"{col}_encoded\"] = encoder.fit_transform(df[col])\n",
    "            encoders[col] = encoder\n",
    "            print(f\"✅ Encoded {col} -> {col}_encoded\")\n",
    "    \n",
    "    # Encode target column jika kategorik\n",
    "    if target_column and df[target_column].dtype == 'object':\n",
    "        target_encoder = LabelEncoder()\n",
    "        df[f\"{target_column}_encoded\"] = target_encoder.fit_transform(df[target_column])\n",
    "        encoders['target'] = target_encoder\n",
    "        target_column = f\"{target_column}_encoded\"\n",
    "        print(f\"✅ Encoded target column: {target_column}\")\n",
    "    \n",
    "    return df, encoders, target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abde4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df, target_column):\n",
    "    \"\"\"Prepare features untuk training\"\"\"\n",
    "    print(\"🎯 Preparing features for training...\")\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_columns = [\n",
    "        target_column,\n",
    "        'id', 'index', 'timestamp', 'date'\n",
    "    ]\n",
    "    \n",
    "    # Add original categorical columns to exclude\n",
    "    categorical_originals = [col for col in df.columns if col.endswith('_encoded')]\n",
    "    for encoded_col in categorical_originals:\n",
    "        original_col = encoded_col.replace('_encoded', '')\n",
    "        if original_col in df.columns:\n",
    "            exclude_columns.append(original_col)\n",
    "    \n",
    "    # Select feature columns\n",
    "    feature_columns = []\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_columns and df[col].dtype in ['int64', 'float64']:\n",
    "            feature_columns.append(col)\n",
    "    \n",
    "    print(f\"📊 Selected features: {feature_columns}\")\n",
    "    print(f\"🎯 Target column: {target_column}\")\n",
    "    \n",
    "    if len(feature_columns) == 0:\n",
    "        raise ValueError(\"❌ No valid numeric features found for training\")\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44cd2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, X_test, y_train, y_test, feature_columns):\n",
    "    \"\"\"Train multiple models dan return best model\"\"\"\n",
    "    print(\"🤖 Training multiple models...\")\n",
    "    \n",
    "    models = {\n",
    "        'RandomForest': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1\n",
    "        ),\n",
    "        'LightGBM': lgb.LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            verbose=-1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Create pipeline dengan StandardScaler\n",
    "    results = {}\n",
    "    trained_models = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"🔄 Training {name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Create pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('classifier', model)\n",
    "            ])\n",
    "            \n",
    "            # Train model\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            y_pred_proba = pipeline.predict_proba(X_test) if hasattr(pipeline, 'predict_proba') else None\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "            f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "            \n",
    "            # Cross validation\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "            \n",
    "            results[name] = {\n",
    "                'accuracy': accuracy,\n",
    "                'f1_weighted': f1_weighted,\n",
    "                'f1_macro': f1_macro,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std(),\n",
    "                'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()\n",
    "            }\n",
    "            \n",
    "            trained_models[name] = pipeline\n",
    "            \n",
    "            print(f\"✅ {name} - Accuracy: {accuracy:.4f}, F1: {f1_weighted:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error training {name}: {e}\")\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(\n",
    "        [name for name in results.keys() if 'error' not in results[name]], \n",
    "        key=lambda x: results[x]['accuracy']\n",
    "    )\n",
    "    \n",
    "    best_model = trained_models[best_model_name]\n",
    "    best_accuracy = results[best_model_name]['accuracy']\n",
    "    \n",
    "    print(f\"🏆 Best model: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
    "    \n",
    "    return best_model, best_model_name, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82377807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_metadata(model, model_name, results, feature_columns, encoders):\n",
    "    \"\"\"Save model dan metadata\"\"\"\n",
    "    print(\"💾 Saving model and metadata...\")\n",
    "    \n",
    "    # Save model dengan skops\n",
    "    model_path = f\"models/best_model_{model_name.lower()}.skops\"\n",
    "    sio.dump(model, model_path)\n",
    "    \n",
    "    # Save encoders\n",
    "    encoders_path = \"models/encoders.pkl\"\n",
    "    with open(encoders_path, 'wb') as f:\n",
    "        pickle.dump(encoders, f)\n",
    "    \n",
    "    # Create comprehensive metadata\n",
    "    metadata = {\n",
    "        'model_name': model_name,\n",
    "        'model_path': model_path,\n",
    "        'encoders_path': encoders_path,\n",
    "        'feature_columns': feature_columns,\n",
    "        'training_timestamp': datetime.now().isoformat(),\n",
    "        'model_performance': results[model_name],\n",
    "        'all_models_performance': results,\n",
    "        'best_accuracy': results[model_name]['accuracy'],\n",
    "        'model_type': 'classification',\n",
    "        'framework': 'sklearn_pipeline'\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_path = \"models/model_metadata.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"✅ Model saved to {model_path}\")\n",
    "    print(f\"✅ Metadata saved to {metadata_path}\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_summary(metadata, data_source):\n",
    "    \"\"\"Create comprehensive training summary untuk monitoring\"\"\"\n",
    "    print(\"📋 Creating training summary...\")\n",
    "    \n",
    "    training_summary = {\n",
    "        \"training_completed\": True,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"data_source\": data_source,\n",
    "        \"best_model\": metadata['model_name'],\n",
    "        \"best_accuracy\": metadata['best_accuracy'],\n",
    "        \"models_trained\": list(metadata['all_models_performance'].keys()),\n",
    "        \"feature_count\": len(metadata['feature_columns']),\n",
    "        \"status\": \"success\",\n",
    "        \"model_path\": metadata['model_path'],\n",
    "        \"metadata_path\": \"models/model_metadata.json\"\n",
    "    }\n",
    "    \n",
    "    # Save training summary\n",
    "    with open(\"monitoring/training_summary.json\", \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "    \n",
    "    # Save untuk CML report\n",
    "    with open(\"results/training_results.json\", \"w\") as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "    \n",
    "    print(\"✅ Training summary created for monitoring integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"🚀 Starting ML Training Pipeline with Evidently Integration\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Setup directories\n",
    "        setup_directories()\n",
    "        \n",
    "        # Load and prepare data\n",
    "        df, data_source = load_and_prepare_data()\n",
    "        \n",
    "        # Create Evidently data quality report\n",
    "        create_evidently_data_report(df, data_source)\n",
    "        \n",
    "        # Encode categorical features\n",
    "        df, encoders, target_column = encode_categorical_features(df)\n",
    "        \n",
    "        if target_column is None:\n",
    "            raise ValueError(\"❌ No target column found\")\n",
    "        \n",
    "        # Prepare features\n",
    "        feature_columns = prepare_features(df, target_column)\n",
    "        \n",
    "        # Prepare training data\n",
    "        X = df[feature_columns]\n",
    "        y = df[target_column]\n",
    "        \n",
    "        print(f\"📊 Training data shape: X={X.shape}, y={y.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        # Train models\n",
    "        best_model, best_model_name, results = train_models(\n",
    "            X_train, X_test, y_train, y_test, feature_columns\n",
    "        )\n",
    "        \n",
    "        # Save model and metadata\n",
    "        metadata = save_model_and_metadata(\n",
    "            best_model, best_model_name, results, feature_columns, encoders\n",
    "        )\n",
    "        \n",
    "        # Create training summary\n",
    "        create_training_summary(metadata, data_source)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"🎉 Training pipeline completed successfully!\")\n",
    "        print(f\"🏆 Best model: {best_model_name}\")\n",
    "        print(f\"📊 Best accuracy: {metadata['best_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training pipeline failed: {e}\")\n",
    "        \n",
    "        # Create error summary\n",
    "        error_summary = {\n",
    "            \"training_completed\": False,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error\": str(e),\n",
    "            \"status\": \"failed\"\n",
    "        }\n",
    "        \n",
    "        with open(\"monitoring/training_summary.json\", \"w\") as f:\n",
    "            json.dump(error_summary, f, indent=2)\n",
    "        \n",
    "        with open(\"results/training_results.json\", \"w\") as f:\n",
    "            json.dump(error_summary, f, indent=2)\n",
    "        \n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
